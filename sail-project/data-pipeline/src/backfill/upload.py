import os
import json
import sys
from dotenv import load_dotenv
from supabase import create_client, Client

# --- é…ç½® ---
load_dotenv()
SUPABASE_URL = os.getenv("SUPABASE_URL") 
SUPABASE_KEY = os.getenv("SUPABASE_SERVICE_KEY")
print(f"Using Supabase URL: {SUPABASE_URL}")
print(f"Using Supabase Key: {'Set' if SUPABASE_KEY else 'Not Set'}")

INPUT_FILENAME = "events_clean.json"
TABLE_NAME = "events_dev" 

INPUT_FILE = os.path.join(os.path.dirname(__file__), "../../data/processed/", INPUT_FILENAME)
BATCH_SIZE = 500

def get_supabase() -> Client:
    if not SUPABASE_URL or not SUPABASE_KEY:
        print("âŒ Error: Missing Supabase Credentials.")
        print("   Please create a '.env' file with SUPABASE_URL and SUPABASE_SERVICE_KEY.")
        sys.exit(1)
    return create_client(SUPABASE_URL, SUPABASE_KEY)

def upload_batch(supabase, batch):
    db_records = []
    
    for item in batch:
        try:
            loc = item.get("location", {})
            start = item.get("start", {})
            
            record = {
                # --- æ ¸å¿ƒä¸»é”® (ç”¨äº Upsert å»é‡) ---
                "source_id": item["source_id"], # ç°åœ¨æˆ‘ä»¬ç¡®ä¿¡è¡¨é‡Œæœ‰è¿™ä¸ªå­—æ®µäº†
                
                # åŸºç¡€ä¿¡æ¯
                "title": item["title"],
                "summary": item["summary"],
                "image_url": item.get("image_url"),
                
                # æ—¶é—´ (Display)
                "start_year": start.get("year"),
                "end_year": None, 
                "precision": start.get("precision"),
                
                # æ—¶é—´ (Compute)
                "start_astro_year": start.get("astro_year"),
                "end_astro_year": None,
                
                # å®Œæ•´æ—¶é—´å¯¹è±¡ (JSONB)
                "start_time_body": start,
                "end_time_body": None,
                
                # ç©ºé—´ (PostGIS)
                "location": f"POINT({loc['lng']} {loc['lat']})",
                
                # ç©ºé—´å…ƒæ•°æ® (æ‰å¹³åŒ–)
                "place_name": loc.get("placeName"),
                "granularity": loc.get("granularity"),
                "certainty": loc.get("certainty"),
                "region_id": loc.get("regionId"),
                
                # å…¶ä»–
                "importance": item["importance"],
                "sources": item["sources"], 
                "pipeline": item.get("pipeline", {})
            }
            
            db_records.append(record)
            
        except Exception as e:
            print(f"âš ï¸ Skipping item {item.get('title', 'Unknown')}: {e}")

    if not db_records:
        return

    # --- æ‰§è¡Œ Upsert ---
    try:
        # on_conflict="source_id": å¦‚æœ source_id å·²å­˜åœ¨ï¼Œåˆ™æ›´æ–°è¯¥è¡Œï¼Œå¦åˆ™æ’å…¥ã€‚
        # ignore_duplicates=False: æˆ‘ä»¬å¸Œæœ›æ›´æ–°ï¼ˆUpdateï¼‰ï¼Œä¾‹å¦‚æ›´æ–°äº†è¯„åˆ†ç®—æ³•åæƒ³åˆ·æ–°æ•°æ®ã€‚
        data = supabase.table(TABLE_NAME).upsert(
            db_records, 
            on_conflict="source_id"
        ).execute()
        
    except Exception as e:
        print(f"âŒ Batch Upload Failed: {e}")

def run():
    print(f"ğŸš€ Starting Upload to Supabase [{TABLE_NAME}]...")
    
    if not os.path.exists(INPUT_FILE):
        print("âŒ Clean data not found. Run 2_process_data.py first.")
        return

    supabase = get_supabase()

    with open(INPUT_FILE, 'r', encoding='utf-8') as f:
        events = json.load(f)
        
    raw_count = len(events)
    print(f"   Loaded {raw_count} events from JSON.")
    
    # --- å…³é”®ä¿®å¤ï¼šåŸºäº source_id å…¨å±€å»é‡ ---
    # ä½¿ç”¨å­—å…¸æ¨å¯¼å¼ï¼šåå‡ºç°çš„ä¼šè¦†ç›–å…ˆå‡ºç°çš„ï¼Œç¡®ä¿ ID å”¯ä¸€
    unique_events_map = {e["source_id"]: e for e in events}
    unique_events = list(unique_events_map.values())
    
    deduped_count = len(unique_events)
    print(f"   ğŸ§¹ Deduplication: Removed {raw_count - deduped_count} duplicates.")
    print(f"   ğŸ¯ Final Target: {deduped_count} unique events.")
    # -------------------------------------
    
    # ä½¿ç”¨å»é‡åçš„ unique_events è¿›è¡Œæ‰¹é‡ä¸Šä¼ 
    for i in range(0, deduped_count, BATCH_SIZE):
        batch = unique_events[i : i + BATCH_SIZE]
        print(f"   ğŸ“¤ Uploading {i} - {min(i+BATCH_SIZE, deduped_count)}...", end="", flush=True)
        upload_batch(supabase, batch)
        print(" âœ…")

    print(f"âœ¨ Upload Complete!")

if __name__ == "__main__":
    run()